<div align="center">

<h2><a href="https://github.com/alibaba/conv-llava">ConvLLaVA: Hierarchical Backbones as Visual Encoder for Large Multimodal Models</a></h2>

[Chunjiang Ge](https://john-ge.github.io/), [Sijie Cheng](https://adacheng.github.io/), Ziming Wang, Jiale Yuan, Yuan Gao

Jun Song, Shiji Song, [Gao Huang](https://www.gaohuang.net/), Bo Zheng

</div>

<p align="center">
    <a href="http://arxiv.org/abs/2405.15738"> 
        <img src="https://img.shields.io/badge/arXiv-2405.15738-b31b1b.svg?logo=arXiv">
    </a>
    <a href="https://huggingface.co/collections/ConvLLaVA/convllava-66519ef0ccdee62544bd19bf"> 
        <img src="https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Models-ffd21e">
    </a>
    <a href="https://huggingface.co/papers/2405.15738"> 
        <img src="https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Paper-ffd21e">
    </a>
    <a href="https://modelscope.cn/organization/ConvLLaVA?tab=model"> 
        <img src="https://img.shields.io/badge/ğŸ¤–%20ModelScope-Models-5f4cf2.svg">
    </a>
    <a href="https://wisemodel.cn/organization/ConvLLaVA"> 
        <img src="https://img.shields.io/badge/WiseModel-Models-571282.svg">
    </a>
    <a href="https://github.com/alibaba/conv-llava/blob/main/asset/WeChat.png"> 
        <img src="https://img.shields.io/badge/WeChat-Group-5ef27f.svg">
    </a>
    <a href="https://github.com/alibaba/conv-llava/stargazers">
        <img alt="GitHub stars" src="https://img.shields.io/github/stars/alibaba/conv-llava?color=ccf" />
    </a>
</p>

<span>[ <a href="README.md"> English </a> | ä¸­æ–‡ ]</span>

## æ‘˜è¦ :bulb:

é«˜åˆ†è¾¨ç‡å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆLMMï¼‰é¢ä¸´è§†è§‰tokenè¿‡å¤šå’Œè§†è§‰å¹³æ–¹å¤æ‚åº¦çš„æŒ‘æˆ˜ã€‚å½“å‰çš„é«˜åˆ†è¾¨ç‡LMMé€šå¸¸èƒ½å¤Ÿè§£å†³äºŒæ¬¡å¤æ‚åº¦é—®é¢˜ï¼Œå´ä¼šç”Ÿæˆè¿‡é‡çš„è§†è§‰tokenã€‚**ç„¶è€Œï¼Œè¿‡å¤šçš„è§†è§‰tokenæ‰æ˜¯æ›´å…³é”®çš„é—®é¢˜ï¼Œå› ä¸ºå®ƒä¼šå¯¼è‡´æ›´æ˜¾è‘—çš„è®¡ç®—å¼€é”€ã€‚** ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ConvLLaVAï¼Œå®ƒé‡‡ç”¨å±‚æ¬¡åŒ–çš„ä¸»å¹²ç½‘ç»œConvNeXtä½œä¸ºLMMçš„è§†è§‰ç¼–ç å™¨ï¼Œä»¥æ›¿ä»£Vision Transformerï¼ˆViTï¼‰ã€‚**ConvLLaVAå°†é«˜åˆ†è¾¨ç‡å›¾åƒå‹ç¼©æˆå¯Œå«ä¿¡æ¯çš„è§†è§‰ç‰¹å¾ï¼Œæœ‰æ•ˆé¿å…äº†ç”Ÿæˆè¿‡å¤šçš„è§†è§‰tokenã€‚** ä¸ºäº†å¢å¼ºConvLLaVAçš„èƒ½åŠ›ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤é¡¹å…³é”®ä¼˜åŒ–æªæ–½ã€‚

- ç”±äºä½åˆ†è¾¨ç‡é¢„è®­ç»ƒçš„ConvNeXtåœ¨ç›´æ¥åº”ç”¨äºé«˜åˆ†è¾¨ç‡æ—¶è¡¨ç°ä¸ä½³ï¼Œ**æˆ‘ä»¬æ›´æ–°å®ƒä»¥å¼¥åˆè¿™ä¸€å·®è·ã€‚**
- æ­¤å¤–ï¼Œç”±äºConvNeXtåŸæœ‰çš„å‹ç¼©æ¯”å¯¹äºæ›´é«˜åˆ†è¾¨ç‡çš„è¾“å…¥æ¥è¯´ä¸è¶³ï¼Œ**æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªæ–°çš„stageï¼Œä»¥è¿›ä¸€æ­¥å‹ç¼©è§†è§‰token**ï¼Œæœ‰æ•ˆå‡å°‘å†—ä½™ã€‚

**è¿™äº›ä¼˜åŒ–ä½¿å¾—ConvLLaVAèƒ½å¤Ÿæ”¯æŒ1536x1536åˆ†è¾¨ç‡çš„è¾“å…¥ï¼ŒåŒæ—¶ä»…ç”Ÿæˆ576ä¸ªè§†è§‰tokenï¼Œå¹¶é€‚åº”ä»»æ„å®½é«˜æ¯”çš„å›¾åƒã€‚** [å®éªŒç»“æœ](#model-zoo)æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ä¸»æµåŸºå‡†æµ‹è¯•ä¸Šä¸æœ€å…ˆè¿›çš„æ¨¡å‹ç›¸æ¯”å–å¾—äº†ç«äº‰æ€§çš„æ€§èƒ½ã€‚

<div align="center">
  <img src="asset/method.png" width=600" />
</div>
<div align="center">
  <figcaption> LLaVA å’Œ ConvLLaVA ç»“æ„ä¸Šçš„å¯¹æ¯”</figcaption>
</div>


[![Collaborations](https://img.shields.io/badge/Welcome-Collaborations-b31b1b.svg)](mailto:gecj20@mails.tsinghua.edu.cn)
å¦‚æœä½ å¯¹å¤šæ¨¡æ€å¤§æ¨¡å‹æ„Ÿå…´è¶£ï¼Œæˆ–è€…ä½ æœ‰å¾ˆå¥½çš„æƒ³æ³•ï¼Œè¯·ä½ è”ç³»æˆ‘ï¼š[Chunjiang Ge](mailto:gecj20@mails.tsinghua.edu.cn).

[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE)
**Usage and License Notices**: This project utilizes certain datasets and checkpoints that are subject to their respective original licenses. Users must comply with all terms and conditions of these original licenses, including but not limited to the [OpenAI Terms of Use](https://openai.com/policies/terms-of-use) for the dataset and the specific licenses for base language models for checkpoints trained using the dataset (e.g. [Llama community license](https://ai.meta.com/llama/license/) for LLaMA-2 and Vicuna-v1.5). This project does not impose any additional constraints beyond those stipulated in the original licenses. Furthermore, users are reminded to ensure that their use of the dataset and checkpoints is in compliance with all applicable laws and regulations.

## å†…å®¹
- [æ‘˜è¦ :bulb:](#æ‘˜è¦-bulb)
- [å†…å®¹](#å†…å®¹)
- [è®¡åˆ’](#è®¡åˆ’)
- [å®‰è£…](#å®‰è£…)
- [æ¨¡å‹åº“](#æ¨¡å‹åº“)
- [æ•°æ®é›†](#æ•°æ®é›†)
- [è®­ç»ƒ](#è®­ç»ƒ)
- [è¯„æµ‹](#è¯„æµ‹)
- [å¼•ç”¨](#å¼•ç”¨)
- [è‡´è°¢](#è‡´è°¢)

## è®¡åˆ’

- [ ] Add [LMMs-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval) supports.
- [ ] Add [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) supports.
- [ ] Add [xtuner](https://github.com/InternLM/xtuner) supports.
- [x] Release weights.
- [ ] Release inference code.

## å®‰è£…

1. Clone this repository and navigate to ConvLLaVA folder
```bash
git clone https://github.com/alibaba/conv-llava
cd conv-llava
```

1. Install Package
```bash
conda create -n convllava python=3.11 -y
conda activate convllava
pip install --upgrade pip  # enable PEP 660 support
pip install -e .
```

3. Install additional packages for training cases
```bash
pip install -e ".[train]"
pip install flash-attn --no-build-isolation
```

## æ¨¡å‹åº“

æˆ‘ä»¬çš„æ¨¡å‹çš„åœ¨ä¸€äº›æµ‹è¯•åŸºå‡†ä¸Šçš„æ€§èƒ½å¦‚ä¸‹ï¼š

<table class="tg"><thead>
  <tr>
    <th class="tg-nrix">Method</th>
    <th class="tg-nrix">Resolution</th>
    <th class="tg-nrix">Visual Tokens</th>
    <th class="tg-nrix">LLM</th>
    <th class="tg-nrix">MME</th>
    <th class="tg-nrix">MMB</th>
    <th class="tg-nrix">SEED</th>
    <th class="tg-nrix">RealWorldQA</th>
    <th class="tg-nrix">MMMU</th>
    <th class="tg-nrix">MMVet</th>
    <th class="tg-nrix">Text</th>
    <th class="tg-nrix">Doc</th>
    <th class="tg-nrix">POPE</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-nrix">ConvLLaVA</td>
    <td class="tg-nrix">768</td>
    <td class="tg-nrix">144</td>
    <td class="tg-nrix">7B</td>
    <td class="tg-nrix">1541</td>
    <td class="tg-nrix">68</td>
    <td class="tg-nrix">68.8</td>
    <td class="tg-nrix">55.9</td>
    <td class="tg-nrix">36.3</td>
    <td class="tg-nrix">44.8</td>
    <td class="tg-nrix">59.1</td>
    <td class="tg-nrix">44.8</td>
    <td class="tg-nrix">87.3</td>
  </tr>
  <tr>
    <td class="tg-nrix">ConvLLaVA</td>
    <td class="tg-nrix">1024</td>
    <td class="tg-nrix">256</td>
    <td class="tg-nrix">7B</td>
    <td class="tg-nrix">1553</td>
    <td class="tg-nrix">68.8</td>
    <td class="tg-nrix">69.3</td>
    <td class="tg-nrix">58.8</td>
    <td class="tg-nrix">35.1</td>
    <td class="tg-nrix">44.4</td>
    <td class="tg-nrix">62.5</td>
    <td class="tg-nrix">48.5</td>
    <td class="tg-nrix">87.7</td>
  </tr>
  <tr>
    <td class="tg-nrix">ConvLLaVA</td>
    <td class="tg-nrix">1536</td>
    <td class="tg-nrix">576</td>
    <td class="tg-nrix">7B</td>
    <td class="tg-nrix">1575</td>
    <td class="tg-nrix">68.7</td>
    <td class="tg-nrix">70.2</td>
    <td class="tg-nrix">59.9</td>
    <td class="tg-nrix">35.8</td>
    <td class="tg-nrix">45.9</td>
    <td class="tg-nrix">65.8</td>
    <td class="tg-nrix">59</td>
    <td class="tg-nrix">87.3</td>
  </tr>
</tbody></table>

<table class="tg"><thead>
  <tr>
    <th class="tg-nrix" rowspan="2">Method</th>
    <th class="tg-nrix" rowspan="2">Resolution</th>
    <th class="tg-nrix" rowspan="2">Visual Tokens</th>
    <th class="tg-nrix" rowspan="2">LLM</th>
    <th class="tg-nrix" colspan="3">RefCOCO</th>
    <th class="tg-nrix" colspan="3">RefCOCO+</th>
    <th class="tg-nrix" colspan="2">RefCOCOg</th>
    <th class="tg-nrix" rowspan="2">Avg</th>
  </tr>
  <tr>
    <th class="tg-nrix">val</th>
    <th class="tg-nrix">test-A</th>
    <th class="tg-nrix">test-B</th>
    <th class="tg-nrix">val</th>
    <th class="tg-nrix">test-A</th>
    <th class="tg-nrix">test-B</th>
    <th class="tg-nrix">val</th>
    <th class="tg-nrix">test</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-nrix">ConvLLaVA</td>
    <td class="tg-nrix">768</td>
    <td class="tg-nrix">144</td>
    <td class="tg-nrix">7B</td>
    <td class="tg-nrix">84.5</td>
    <td class="tg-nrix">89.0</td>
    <td class="tg-nrix">79.2</td>
    <td class="tg-nrix">77.7</td>
    <td class="tg-nrix">84.9</td>
    <td class="tg-nrix">69.7</td>
    <td class="tg-nrix">79.8</td>
    <td class="tg-nrix">79.7</td>
    <td class="tg-nrix">80.6</td>
  </tr>
  <tr>
    <td class="tg-nrix">ConvLLaVA</td>
    <td class="tg-nrix">1024</td>
    <td class="tg-nrix">256</td>
    <td class="tg-nrix">7B</td>
    <td class="tg-nrix">85.5</td>
    <td class="tg-nrix">89.6</td>
    <td class="tg-nrix">78.8</td>
    <td class="tg-nrix">79.3</td>
    <td class="tg-nrix">86.1</td>
    <td class="tg-nrix">70.3</td>
    <td class="tg-nrix">80.6</td>
    <td class="tg-nrix">81.2</td>
    <td class="tg-nrix">81.4</td>
  </tr>
  <tr>
    <td class="tg-nrix">ConvLLaVA</td>
    <td class="tg-nrix">1536</td>
    <td class="tg-nrix">576</td>
    <td class="tg-nrix">7B</td>
    <td class="tg-nrix">86.5</td>
    <td class="tg-nrix">90.6</td>
    <td class="tg-nrix">80.5</td>
    <td class="tg-nrix">80.0</td>
    <td class="tg-nrix">86.8</td>
    <td class="tg-nrix">71.5</td>
    <td class="tg-nrix">82.0</td>
    <td class="tg-nrix">82.4</td>
    <td class="tg-nrix">82.3</td>
  </tr>
</tbody></table>

æˆ‘ä»¬çš„ [Model Zoo](https://github.com/alibaba/conv-llava/blob/main/docs/Model_zoo.md) ä¸­åŒ…å«äº†ä¸»è¦çš„æƒé‡å’Œä¸‹è½½æ–¹å¼ï¼Œå¹¶æœ‰è¯´æ˜å¦‚ä½•ä½¿ç”¨è¿™äº›æƒé‡ã€‚

## æ•°æ®é›†

æˆ‘ä»¬å®éªŒç”¨åˆ°çš„æ•°æ®åœ¨ [Data.md](https://github.com/alibaba/conv-llava/blob/main/docs/Data.md) ä¸­æœ‰ä»‹ç»ã€‚

## è®­ç»ƒ

è®­ç»ƒçš„è¶…å‚æ•°å¦‚ä¸‹ï¼š

| Hyperparameters | Stage 1 | Stage 2 | Stage 3 |
| --------------- | ------- | ------- | ------- |
| Learning Rate   | 3e-4    | 2e-5    | 2e-5    |
| Batch Size      | 256     | 256     | 128     |
| Epochs          | 1       | 1       | 1       |
| Warmup Ratio    | 0.03    | 0.03    | 0.03    |
| Weight Decay    | 0       | 0       | 0       |
| Optimizer       | AdamW   | AdamW   | AdamW   |

è®­ç»ƒè„šæœ¬åœ¨æ–‡ä»¶å¤¹ [scripts](https://github.com/alibaba/conv-llava/tree/main/scripts) ä¸­:

- Projector Initialzation: [stage1](https://github.com/alibaba/conv-llava/tree/main/scripts/stage_1.sh)
- Vision Language Pretraining: [stage2](https://github.com/alibaba/conv-llava/tree/main/scripts/stage_2.sh)
- Instruction Tuning: [stage3](https://github.com/alibaba/conv-llava/tree/main/scripts/stage_3.sh)

## è¯„æµ‹

æˆ‘ä»¬ç›®å‰æ”¯æŒ [VLMEVALKIT](https://github.com/open-compass/VLMEvalKit) å’Œ [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval) æ¥æµ‹è¯•æ¨¡å‹ã€‚è¯·çœ‹ [Evaluation.md](https://github.com/alibaba/conv-llava/blob/main/docs/Evaluation.md) äº†è§£æ›´å¤šç»†èŠ‚.

## å¼•ç”¨

å¦‚æœä½ è®¤ä¸ºæˆ‘ä»¬çš„å·¥ä½œæœ‰æ‰€å¸®åŠ©ï¼Œè¯·ä½ é€šè¿‡ä¸‹é¢çš„ BibTeX æ¥å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œ:

```bibtex
@misc{ge2024convllava,
    title={ConvLLaVA: Hierarchical Backbones as Visual
Encoder for Large Multimodal Models},
    author={Chunjiang Ge, Sijie Cheng, Ziming Wang, Jiale Yuan, Yuan Gao, Jun Song, Shiji Song, Gao Huang, Bo Zheng},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
    year={2024}
    eprint={2045.15738},
}
```

## è‡´è°¢

- [Vicuna](https://github.com/lm-sys/FastChat): the codebase LLaVA built upon, and our base model Vicuna-13B that has the amazing language capabilities!
- [LLaVA](https://github.com/haotian-liu/LLaVA): the codebase we built upon.
